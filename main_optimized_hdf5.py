#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆHDF5å¢é‡å¤„ç†ç³»ç»Ÿ - 2850-2950ftæ·±åº¦èŒƒå›´ä¸“ç”¨
å†…å­˜é«˜æ•ˆçš„æµ‹äº•æ•°æ®çªœæ§½æ£€æµ‹å¤„ç†æµç¨‹
"""

import sys
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging
import time
import gc
from typing import Dict, List, Tuple

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('src')

from src.feature_engineering import IncrementalFeatureEngineer
from src.model import HybridChannelingModel
from src.hdf5_manager import HDF5DataManager
from src.data_loader import DataLoader
from src.signal_processing import SignalProcessor
from src.plot_config import setup_matplotlib

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimized_hdf5_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def process_single_receiver_sector(data_loader: DataLoader, 
                                  signal_processor: SignalProcessor,
                                  filtered_cast: Dict,
                                  filtered_xsilmr: Dict,
                                  receiver_id: int = 7,
                                  sector_idx: int = 0) -> Tuple[int, str]:
    """
    å¤„ç†å•ä¸ªæ¥æ”¶å™¨çš„å•ä¸ªæ–¹ä½æ‰‡åŒº
    
    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        signal_processor: ä¿¡å·å¤„ç†å™¨
        filtered_cast: ç­›é€‰åçš„CASTæ•°æ®
        filtered_xsilmr: ç­›é€‰åçš„XSILMRæ•°æ®
        receiver_id: æ¥æ”¶å™¨ID
        sector_idx: æ‰‡åŒºç´¢å¼•
        
    Returns:
        (æ ·æœ¬æ•°, HDF5æ–‡ä»¶è·¯å¾„)
    """
    # é…ç½®å‚æ•°
    depth_range = (2850.0, 2950.0)
    hdf5_path = f"data/processed/receiver_{receiver_id}_sector_{sector_idx}_features.h5"
    
    logger.info(f"å¤„ç†æ¥æ”¶å™¨ {receiver_id} æ‰‡åŒº {sector_idx}...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path("data/processed").mkdir(parents=True, exist_ok=True)
    
    # åˆ é™¤æ—§æ–‡ä»¶
    if Path(hdf5_path).exists():
        Path(hdf5_path).unlink()
    
    # è·å–æ–¹ä½è§’æ•°æ®
    side_keys = ['SideA', 'SideB', 'SideC', 'SideD', 
                'SideE', 'SideF', 'SideG', 'SideH']
    
    if receiver_id not in filtered_xsilmr:
        logger.error(f"æ¥æ”¶å™¨ {receiver_id} ä¸å­˜åœ¨")
        return 0, hdf5_path
    
    side_key = side_keys[sector_idx]
    if side_key not in filtered_xsilmr[receiver_id]:
        logger.error(f"æ¥æ”¶å™¨ {receiver_id} çš„ {side_key} æ•°æ®ä¸å­˜åœ¨")
        return 0, hdf5_path
    
    # è·å–æ•°æ®
    waveforms = filtered_xsilmr[receiver_id][side_key]  # (1024, n_depths)
    depths = filtered_xsilmr[receiver_id]['AbsoluteDepth']
    
    # åˆ›å»ºæ ‡ç­¾
    cast_depths = filtered_cast['Depth']
    cast_zc = filtered_cast['Zc']
    
    # è®¡ç®—æ–¹ä½è§’èŒƒå›´
    sector_size = 360 // 8  # 45åº¦æ¯æ‰‡åŒº
    start_angle = sector_idx * sector_size
    end_angle = (sector_idx + 1) * sector_size
    
    # æ–¹ä½è§’ç´¢å¼• (æ¯2åº¦ä¸€ä¸ª)
    start_idx = start_angle // 2
    end_idx = end_angle // 2
    
    # è·å–æ‰‡åŒºæ•°æ®
    azimuth_zc = cast_zc[start_idx:end_idx, :]
    channeling_mask = azimuth_zc < 2.5
    channeling_ratios = np.mean(channeling_mask, axis=0)
    
    # æ’å€¼åˆ°XSILMRæ·±åº¦
    sector_labels = np.interp(depths, cast_depths, channeling_ratios)
    
    # è®¡ç®—å®é™…æ ·æœ¬æ•°
    total_samples = min(len(depths), len(sector_labels), waveforms.shape[1])
    
    if total_samples == 0:
        logger.warning(f"æ¥æ”¶å™¨ {receiver_id} æ‰‡åŒº {sector_idx} æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
        return 0, hdf5_path
    
    logger.info(f"é¢„æœŸå¤„ç† {total_samples} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºHDF5ç®¡ç†å™¨
    manager = HDF5DataManager(hdf5_path, mode='w')
    manager.create_dataset_structure(
        total_samples=total_samples,
        image_shape=(127, 1024),
        vector_dim=8,
        chunk_size=min(50, total_samples)
    )
    
    # åˆ›å»ºæ‰¹å¤„ç†å™¨
    from src.hdf5_manager import BatchProcessor
    batch_processor = BatchProcessor(manager, batch_size=min(20, total_samples))
    
    # å¤„ç†æ•°æ®
    processed_count = 0
    for depth_idx in range(total_samples):
        try:
            # æå–æ³¢å½¢
            waveform = waveforms[:, depth_idx]  # (1024,)
            
            # ä¿¡å·å¤„ç†
            filtered_waveform = signal_processor.apply_highpass_filter(waveform)
            scalogram = signal_processor.generate_scalogram(filtered_waveform)
            physical_features = signal_processor.extract_physical_features(filtered_waveform)
            
            # åˆ›å»ºç‰¹å¾å‘é‡
            vector_features = np.array([
                physical_features['max_amplitude'],
                physical_features['rms_amplitude'],
                physical_features['energy'],
                physical_features['zero_crossings'],
                physical_features['dominant_frequency'],
                physical_features['spectral_centroid'],
                receiver_id,
                sector_idx
            ], dtype=np.float32)
            
            # æ ‡ç­¾å’Œå…ƒæ•°æ®
            label = sector_labels[depth_idx]
            metadata = (depths[depth_idx], receiver_id, sector_idx, depth_idx)
            
            # æ·»åŠ åˆ°æ‰¹å¤„ç†å™¨
            batch_processor.add_sample(scalogram, vector_features, label, metadata)
            
            processed_count += 1
            
            if processed_count % 50 == 0:
                logger.info(f"  å·²å¤„ç† {processed_count}/{total_samples} ä¸ªæ·±åº¦ç‚¹")
                
        except Exception as e:
            logger.warning(f"å¤„ç†æ·±åº¦ç‚¹ {depth_idx} æ—¶å‡ºé”™: {e}")
            continue
    
    # å®Œæˆå¤„ç†
    actual_samples = batch_processor.finalize()
    
    # æ¸…ç†å†…å­˜
    del batch_processor
    del manager
    gc.collect()
    
    logger.info(f"æ¥æ”¶å™¨ {receiver_id} æ‰‡åŒº {sector_idx} å¤„ç†å®Œæˆ: {actual_samples} æ ·æœ¬")
    
    return actual_samples, hdf5_path

def merge_hdf5_files(file_paths: List[str], output_path: str) -> int:
    """
    åˆå¹¶å¤šä¸ªHDF5æ–‡ä»¶åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­
    
    Args:
        file_paths: è¾“å…¥HDF5æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_path: è¾“å‡ºHDF5æ–‡ä»¶è·¯å¾„
        
    Returns:
        åˆå¹¶åçš„æ€»æ ·æœ¬æ•°
    """
    logger.info(f"åˆå¹¶ {len(file_paths)} ä¸ªHDF5æ–‡ä»¶...")
    
    # è®¡ç®—æ€»æ ·æœ¬æ•°
    total_samples = 0
    for file_path in file_paths:
        if Path(file_path).exists():
            manager = HDF5DataManager(file_path, mode='r')
            info = manager.get_data_info()
            total_samples += info['total_samples']
            logger.info(f"  {file_path}: {info['total_samples']} æ ·æœ¬")
    
    if total_samples == 0:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬å¯ä»¥åˆå¹¶")
        return 0
    
    logger.info(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
    output_manager = HDF5DataManager(output_path, mode='w')
    output_manager.create_dataset_structure(
        total_samples=total_samples,
        image_shape=(127, 1024),
        vector_dim=8,
        chunk_size=min(100, total_samples)
    )
    
    # åˆå¹¶æ•°æ®
    current_idx = 0
    for file_path in file_paths:
        if not Path(file_path).exists():
            continue
            
        # è¯»å–æºæ–‡ä»¶
        source_manager = HDF5DataManager(file_path, mode='r')
        source_info = source_manager.get_data_info()
        n_samples = source_info['total_samples']
        
        if n_samples == 0:
            continue
        
        # åˆ†æ‰¹è¯»å–å’Œå†™å…¥
        batch_size = min(100, n_samples)
        for start_idx in range(0, n_samples, batch_size):
            end_idx = min(start_idx + batch_size, n_samples)
            batch_data = source_manager.read_batch(start_idx, end_idx - start_idx)
            
            # å†™å…¥åˆ°è¾“å‡ºæ–‡ä»¶
            output_manager.write_batch(
                current_idx,
                batch_data['image_features'],
                batch_data['vector_features'],
                batch_data['labels'],
                batch_data.get('metadata')
            )
            
            current_idx += len(batch_data['image_features'])
        
        logger.info(f"  å·²åˆå¹¶ {file_path}: {n_samples} æ ·æœ¬")
    
    # æ¸…ç†
    del output_manager
    gc.collect()
    
    logger.info(f"åˆå¹¶å®Œæˆ: æ€»å…± {current_idx} æ ·æœ¬")
    return current_idx

def main():
    """ä¸»å¤„ç†æµç¨‹"""
    print("="*80)
    print("ğŸš€ ä¼˜åŒ–ç‰ˆHDF5å¢é‡å¤„ç†ç³»ç»Ÿ")
    print("="*80)
    print("ğŸ“‹ å¤„ç†èŒƒå›´: 2850-2950ft (100ftå®Œæ•´æ·±åº¦èŒƒå›´)")
    print("ğŸ”§ å¤„ç†ç­–ç•¥: å…¨æ¥æ”¶å™¨(1-13) + å…¨æ–¹ä½è§’(A-H) + æœ€ç»ˆåˆå¹¶")
    print("âš¡ ä¼˜åŒ–ç›®æ ‡: å†…å­˜é«˜æ•ˆ + æ•°æ®å®Œæ•´æ€§ + æœ€å¤§è¦†ç›–åº¦")
    print("="*80)
    
    try:
        # é…ç½®matplotlib
        setup_matplotlib()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dirs = ['data/processed', 'outputs/models', 'outputs/figures', 'outputs/logs']
        for dir_path in output_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        start_time = time.time()
        
        # 1. åŠ è½½å’Œç­›é€‰æ•°æ®
        logger.info("æ­¥éª¤1: åŠ è½½å’Œç­›é€‰åŸºç¡€æ•°æ®")
        print("\nğŸ“Š æ­¥éª¤1: æ•°æ®åŠ è½½å’Œç­›é€‰")
        print("-" * 60)
        
        data_loader = DataLoader()
        signal_processor = SignalProcessor()
        
        # åŠ è½½æ•°æ®
        cast_data = data_loader.load_cast_data()
        xsilmr_data = data_loader.load_xsilmr_data()
        
        # ç­›é€‰æ·±åº¦èŒƒå›´
        filtered_cast, filtered_xsilmr = data_loader.filter_depth_range(
            min_depth=2850.0,
            max_depth=2950.0
        )
        
        # è®¡ç®—ç»å¯¹æ·±åº¦
        filtered_xsilmr = data_loader.calculate_absolute_depths(filtered_xsilmr)
        
        # è·å–ä¸€ä¸ªå¯ç”¨æ¥æ”¶å™¨çš„æ·±åº¦ç‚¹æ•°ä½œä¸ºå‚è€ƒ
        reference_receiver = None
        for r_id in range(1, 14):
            if r_id in filtered_xsilmr:
                reference_receiver = r_id
                break
        
        xsilmr_depth_count = len(filtered_xsilmr[reference_receiver]['Depth']) if reference_receiver else 0
        logger.info(f"ç­›é€‰åæ•°æ®: CAST={len(filtered_cast['Depth'])}ç‚¹, XSILMR={xsilmr_depth_count}ç‚¹ (åŸºäºæ¥æ”¶å™¨{reference_receiver})")
        
        # 2. åˆ†å—å¤„ç†
        logger.info("æ­¥éª¤2: åˆ†å—å¢é‡å¤„ç†")
        print("\nğŸ”„ æ­¥éª¤2: åˆ†å—å¢é‡å¤„ç†")
        print("-" * 60)
        
        # å¤„ç†é…ç½®ï¼šå¤„ç†æ‰€æœ‰æ¥æ”¶å™¨å’Œæ‰€æœ‰æ–¹ä½
        target_receivers = list(range(1, 14))  # æ‰€æœ‰13ä¸ªæ¥æ”¶å™¨ (1-13)
        target_sectors = list(range(8))  # æ‰€æœ‰8ä¸ªæ–¹ä½æ‰‡åŒº (A-H)
        
        hdf5_files = []
        total_processed_samples = 0
        
        for receiver_id in target_receivers:
            for sector_idx in target_sectors:
                print(f"  å¤„ç†æ¥æ”¶å™¨ {receiver_id} æ–¹ä½æ‰‡åŒº {sector_idx}...")
                
                samples, file_path = process_single_receiver_sector(
                    data_loader, signal_processor,
                    filtered_cast, filtered_xsilmr,
                    receiver_id, sector_idx
                )
                
                if samples > 0:
                    hdf5_files.append(file_path)
                    total_processed_samples += samples
                    print(f"    âœ… æˆåŠŸå¤„ç† {samples} æ ·æœ¬")
                else:
                    print(f"    âš ï¸ è·³è¿‡ï¼ˆæ— æœ‰æ•ˆæ ·æœ¬ï¼‰")
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
        
        processing_time = time.time() - start_time
        
        # 3. åˆå¹¶HDF5æ–‡ä»¶
        logger.info("æ­¥éª¤3: åˆå¹¶HDF5æ–‡ä»¶")
        print("\nğŸ”— æ­¥éª¤3: åˆå¹¶HDF5æ–‡ä»¶")
        print("-" * 60)
        
        if len(hdf5_files) > 0:
            final_hdf5_path = "data/processed/final_features_2850_2950.h5"
            
            # åˆ é™¤æ—§çš„åˆå¹¶æ–‡ä»¶
            if Path(final_hdf5_path).exists():
                Path(final_hdf5_path).unlink()
            
            merged_samples = merge_hdf5_files(hdf5_files, final_hdf5_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for file_path in hdf5_files:
                if Path(file_path).exists():
                    Path(file_path).unlink()
                    logger.info(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            
            print(f"âœ… æˆåŠŸåˆå¹¶ {merged_samples} æ ·æœ¬åˆ° {final_hdf5_path}")
            
            # 4. æ£€æŸ¥åˆå¹¶åçš„æ•°æ®
            logger.info("æ­¥éª¤4: éªŒè¯åˆå¹¶æ•°æ®")
            print("\nğŸ” æ­¥éª¤4: æ•°æ®éªŒè¯")
            print("-" * 60)
            
            final_manager = HDF5DataManager(final_hdf5_path, mode='r')
            final_info = final_manager.get_data_info()
            
            print(f"ğŸ“‹ æœ€ç»ˆæ•°æ®é›†ä¿¡æ¯:")
            print(f"   â€¢ æ€»æ ·æœ¬æ•°: {final_info['total_samples']:,}")
            print(f"   â€¢ æ–‡ä»¶å¤§å°: {final_info['file_size_mb']:.1f} MB")
            print(f"   â€¢ å›¾åƒç‰¹å¾å½¢çŠ¶: {final_info['image_shape']}")
            print(f"   â€¢ æ•°å€¼ç‰¹å¾ç»´åº¦: {final_info['vector_dim']}")
            
            # è¯»å–æ ·æœ¬è¿›è¡Œç»Ÿè®¡
            sample_data = final_manager.read_batch(0, min(100, final_info['total_samples']))
            labels = sample_data['labels']
            
            print(f"ğŸ“Š æ ‡ç­¾ç»Ÿè®¡ (åŸºäºå‰{len(labels)}ä¸ªæ ·æœ¬):")
            print(f"   â€¢ å¹³å‡çªœæ§½æ¯”ä¾‹: {np.mean(labels):.3f}")
            print(f"   â€¢ æ ‡å‡†å·®: {np.std(labels):.3f}")
            print(f"   â€¢ æœ€å°å€¼: {np.min(labels):.3f}")
            print(f"   â€¢ æœ€å¤§å€¼: {np.max(labels):.3f}")
            print(f"   â€¢ ä¸­åº¦çªœæ§½æ ·æœ¬ (â‰¥50%): {np.mean(labels >= 0.5):.1%}")
            
            # 5. æ¨¡å‹è®­ç»ƒæ¼”ç¤ºï¼ˆå¯é€‰ï¼‰
            print("\nğŸ¤– æ­¥éª¤5: æ¨¡å‹è®­ç»ƒæ¼”ç¤º")
            print("-" * 60)
            
            # åˆ›å»ºæ¨¡å‹
            model = HybridChannelingModel(image_shape=(127, 1024), vector_dim=8)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰
            print("å¼€å§‹æ¨¡å‹è®­ç»ƒï¼ˆæ¼”ç¤ºç‰ˆï¼š10ä¸ªepochï¼‰...")
            training_start = time.time()
            
            history = model.train_from_hdf5(
                hdf5_path=final_hdf5_path,
                epochs=10,  # å¿«é€Ÿæ¼”ç¤º
                batch_size=16,
                test_size=0.2,
                val_size=0.1
            )
            
            training_time = time.time() - training_start
            
            # ä¿å­˜æ¨¡å‹
            model.save_model(
                'outputs/models/optimized_channeling_model.h5',
                'outputs/models/optimized_scaler.pkl'
            )
            
            # ç»˜åˆ¶è®­ç»ƒå†å²
            model.plot_training_history('outputs/figures/optimized_training_history.png')
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.1f}ç§’")
            
            # å¯¼å‡ºæœ€ç»ˆæ‘˜è¦
            final_manager.export_summary('outputs/logs/final_dataset_summary.txt')
            
        else:
            logger.error("æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ•°æ®ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # æ€»ç»“æŠ¥å‘Š
        total_time = time.time() - start_time
        
        print("\n" + "="*80)
        print("ğŸ‰ ä¼˜åŒ–ç‰ˆHDF5å¢é‡å¤„ç†å®Œæˆï¼")
        print("="*80)
        
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   â€¢ å¤„ç†æ·±åº¦èŒƒå›´: 2850-2950ft")
        print(f"   â€¢ æ¥æ”¶å™¨æ•°é‡: {len(target_receivers)} ä¸ª (R1-R13)")
        print(f"   â€¢ æ–¹ä½æ‰‡åŒºæ•°é‡: {len(target_sectors)} ä¸ª")
        print(f"   â€¢ æ€»å¤„ç†ç»„åˆ: {len(target_receivers) * len(target_sectors)} ä¸ª")
        print(f"   â€¢ æ€»æ ·æœ¬æ•°: {merged_samples:,}")
        print(f"   â€¢ å¤„ç†è€—æ—¶: {processing_time:.1f}ç§’")
        print(f"   â€¢ è®­ç»ƒè€—æ—¶: {training_time:.1f}ç§’")
        print(f"   â€¢ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   â€¢ æœ€ç»ˆæ•°æ®é›†: {final_hdf5_path}")
        print(f"   â€¢ è®­ç»ƒæ¨¡å‹: outputs/models/optimized_channeling_model.h5")
        print(f"   â€¢ æ•°æ®æ‘˜è¦: outputs/logs/final_dataset_summary.txt")
        print(f"   â€¢ è®­ç»ƒå›¾è¡¨: outputs/figures/optimized_training_history.png")
        
        print(f"\nğŸ¯ å…³é”®ä¼˜åŠ¿:")
        print(f"   âœ“ å†…å­˜é«˜æ•ˆ: åˆ†å—å¤„ç†é¿å…å†…å­˜æº¢å‡º")
        print(f"   âœ“ å…¨é¢è¦†ç›–: å¤„ç†å…¨éƒ¨13ä¸ªæ¥æ”¶å™¨çš„8ä¸ªæ–¹ä½ç»„åˆ")
        print(f"   âœ“ æ•°æ®å‹ç¼©: HDF5æ ¼å¼å¤§å¹…å‡å°‘å­˜å‚¨ç©ºé—´")
        print(f"   âœ“ å¯æ‰©å±•æ€§: æ”¯æŒä»»æ„æ·±åº¦èŒƒå›´å’Œæ¥æ”¶å™¨ç»„åˆ")
        print(f"   âœ“ æ¨¡å‹è®­ç»ƒ: ç›´æ¥ä»HDF5æµå¼è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹")
        
        return True
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # æ¸…ç†å†…å­˜
        gc.collect()

if __name__ == "__main__":
    print("ä¼˜åŒ–ç‰ˆæµ‹äº•æ•°æ®HDF5å¢é‡å¤„ç†ç³»ç»Ÿ")
    print("="*50)
    
    success = main()
    
    if success:
        print("\nğŸ‰ å¤„ç†æˆåŠŸå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼")
        sys.exit(1) 