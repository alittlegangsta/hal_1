#!/usr/bin/env python3
"""
çªœæ§½æ£€æµ‹å®Œæ•´æµæ°´çº¿
ä¸€é”®è¿è¡Œä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒåˆ°ç»“æœè¾“å‡ºçš„å®Œæ•´æµç¨‹
"""
import os
import sys
import logging
import time
import argparse
from pathlib import Path
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
import platform

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    system = platform.system()
    
    if system == "Darwin":  # macOS
        chinese_fonts = ['PingFang SC', 'Arial Unicode MS', 'Hiragino Sans GB', 'STHeiti', 'SimHei']
    elif system == "Windows":
        chinese_fonts = ['Microsoft YaHei', 'SimHei', 'KaiTi', 'SimSun']
    else:  # Linux
        chinese_fonts = ['WenQuanYi Micro Hei', 'DejaVu Sans', 'SimHei']
    
    for font in chinese_fonts:
        try:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            # æµ‹è¯•å­—ä½“
            test_fig = plt.figure(figsize=(1, 1))
            plt.text(0.5, 0.5, 'æµ‹è¯•', fontsize=12)
            plt.close(test_fig)
            print(f"æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font}")
            return True
        except:
            continue
    
    print("è­¦å‘Š: æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“")
    return False

def setup_logging(output_dir: Path) -> str:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_dir = output_dir / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"pipeline_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return str(log_file)

class CompletePipeline:
    """å®Œæ•´çš„çªœæ§½æ£€æµ‹æµæ°´çº¿"""
    
    def __init__(self, 
                 depth_range=(2850.0, 2950.0),
                 data_scale="small",
                 output_dir="outputs"):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            depth_range: æ·±åº¦èŒƒå›´ (ft)
            data_scale: æ•°æ®è§„æ¨¡ ("small", "medium", "large")
            output_dir: è¾“å‡ºç›®å½•
        """
        self.depth_range = depth_range
        self.data_scale = data_scale
        self.output_dir = Path(output_dir)
        
        # æ ¹æ®æ•°æ®è§„æ¨¡è®¾ç½®å‚æ•°
        self.scale_config = {
            "small": {
                "receivers": [1],
                "azimuth_sectors": 2,
                "max_depths_per_sector": 50,
                "batch_size": 20,
                "epochs": 30,
                "description": "å¿«é€Ÿæµ‹è¯• (1ä¸ªæ¥æ”¶å™¨, 2ä¸ªæ‰‡åŒº, 50ä¸ªæ·±åº¦ç‚¹/æ‰‡åŒº)"
            },
            "medium": {
                "receivers": [1, 2, 3, 4, 5],
                "azimuth_sectors": 4,
                "max_depths_per_sector": 100,
                "batch_size": 50,
                "epochs": 50,
                "description": "ä¸­ç­‰è§„æ¨¡ (5ä¸ªæ¥æ”¶å™¨, 4ä¸ªæ‰‡åŒº, 100ä¸ªæ·±åº¦ç‚¹/æ‰‡åŒº)"
            },
            "large": {
                "receivers": list(range(1, 14)),  # 1-13
                "azimuth_sectors": 8,
                "max_depths_per_sector": None,  # å…¨éƒ¨æ·±åº¦ç‚¹
                "batch_size": 100,
                "epochs": 100,
                "description": "å®Œæ•´è§„æ¨¡ (13ä¸ªæ¥æ”¶å™¨, 8ä¸ªæ‰‡åŒº, å…¨éƒ¨æ·±åº¦ç‚¹)"
            }
        }
        
        self.config = self.scale_config[data_scale]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "figures").mkdir(exist_ok=True)
        (self.output_dir / "models").mkdir(exist_ok=True)
        (self.output_dir / "data").mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.log_file = setup_logging(self.output_dir)
        self.logger = logging.getLogger(__name__)
        
        self.logger.info("=" * 80)
        self.logger.info("çªœæ§½æ£€æµ‹å®Œæ•´æµæ°´çº¿åˆå§‹åŒ–")
        self.logger.info("=" * 80)
        self.logger.info(f"æ·±åº¦èŒƒå›´: {depth_range[0]}-{depth_range[1]} ft")
        self.logger.info(f"æ•°æ®è§„æ¨¡: {data_scale} - {self.config['description']}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        self.logger.info("=" * 80)
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        start_time = time.time()
        
        try:
            # é˜¶æ®µ1: æ•°æ®å¤„ç†
            hdf5_path = self.stage1_data_processing()
            
            # é˜¶æ®µ2: æ•°æ®åˆ†æ
            self.stage2_data_analysis(hdf5_path)
            
            # é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ
            model, history = self.stage3_model_training(hdf5_path)
            
            # é˜¶æ®µ4: ç»“æœè¾“å‡º
            self.stage4_result_output(model, history, hdf5_path)
            
            total_time = time.time() - start_time
            self.logger.info("=" * 80)
            self.logger.info("ğŸ‰ å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ!")
            self.logger.info(f"æ€»è€—æ—¶: {total_time/60:.2f} åˆ†é’Ÿ")
            self.logger.info("=" * 80)
            
            return True
            
        except Exception as e:
            self.logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False
    
    def stage1_data_processing(self) -> str:
        """é˜¶æ®µ1: æ•°æ®å¤„ç† - HDF5å¢é‡å­˜å‚¨"""
        self.logger.info("\n" + "="*60)
        self.logger.info("é˜¶æ®µ1: æ•°æ®å¤„ç† - HDF5å¢é‡å­˜å‚¨")
        self.logger.info("="*60)
        
        from src.data_loader import DataLoader
        from src.signal_processing import SignalProcessor
        from src.hdf5_manager import HDF5DataManager, BatchProcessor
        import gc
        
        # HDF5æ–‡ä»¶è·¯å¾„
        hdf5_path = self.output_dir / "data" / f"features_{self.data_scale}_{int(self.depth_range[0])}_{int(self.depth_range[1])}.h5"
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        self.logger.info("åŠ è½½åŸå§‹æ•°æ®...")
        data_loader = DataLoader()
        cast_data = data_loader.load_cast_data()
        xsilmr_data = data_loader.load_xsilmr_data()
        
        # 2. ç­›é€‰æ·±åº¦èŒƒå›´
        self.logger.info(f"ç­›é€‰æ·±åº¦èŒƒå›´: {self.depth_range[0]}-{self.depth_range[1]} ft")
        filtered_cast, filtered_xsilmr = data_loader.filter_depth_range(
            min_depth=self.depth_range[0], max_depth=self.depth_range[1]
        )
        filtered_xsilmr = data_loader.calculate_absolute_depths(filtered_xsilmr)
        
        # 3. ä¼°ç®—æ ·æœ¬æ•°
        total_samples = self._estimate_samples(filtered_xsilmr)
        self.logger.info(f"é¢„ä¼°æ ·æœ¬æ•°: {total_samples}")
        
        # 4. åˆ›å»ºHDF5æ–‡ä»¶
        hdf5_manager = HDF5DataManager(str(hdf5_path), mode='w')
        hdf5_manager.create_dataset_structure(
            total_samples=total_samples,
            image_shape=(127, 1024),
            vector_dim=8,
            chunk_size=self.config['batch_size']
        )
        
        batch_processor = BatchProcessor(hdf5_manager, batch_size=self.config['batch_size'])
        signal_processor = SignalProcessor()
        
        # 5. å¤„ç†æ•°æ®
        actual_samples = self._process_receivers(
            filtered_cast, filtered_xsilmr, batch_processor, signal_processor
        )
        
        # 6. å®Œæˆå¤„ç†
        final_count = batch_processor.finalize()
        hdf5_manager.close()
        
        self.logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: é¢„æœŸ{total_samples}ä¸ªï¼Œå®é™…ç”Ÿæˆ{final_count}ä¸ªæ ·æœ¬")
        self.logger.info(f"HDF5æ–‡ä»¶: {hdf5_path}")
        self.logger.info(f"æ–‡ä»¶å¤§å°: {hdf5_path.stat().st_size / (1024**2):.2f} MB")
        
        return str(hdf5_path)
    
    def _estimate_samples(self, filtered_xsilmr: dict) -> int:
        """ä¼°ç®—æ ·æœ¬æ•°"""
        total_samples = 0
        
        for receiver_id in self.config['receivers']:
            if receiver_id not in filtered_xsilmr:
                continue
            
            receiver_data = filtered_xsilmr[receiver_id]
            n_depths = len(receiver_data['AbsoluteDepth'])
            
            if self.config['max_depths_per_sector']:
                n_depths = min(n_depths, self.config['max_depths_per_sector'])
            
            samples_per_receiver = self.config['azimuth_sectors'] * n_depths
            total_samples += samples_per_receiver
        
        return total_samples
    
    def _process_receivers(self, cast_data, xsilmr_data, batch_processor, signal_processor):
        """å¤„ç†æ‰€æœ‰æ¥æ”¶å™¨çš„æ•°æ®"""
        sample_count = 0
        
        for receiver_id in self.config['receivers']:
            if receiver_id not in xsilmr_data:
                self.logger.warning(f"æ¥æ”¶å™¨{receiver_id}ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            self.logger.info(f"å¤„ç†æ¥æ”¶å™¨ {receiver_id}")
            receiver_data = xsilmr_data[receiver_id]
            depths = receiver_data['AbsoluteDepth']
            
            for sector_idx in range(self.config['azimuth_sectors']):
                self.logger.info(f"  æ–¹ä½è§’æ‰‡åŒº {sector_idx + 1}/{self.config['azimuth_sectors']}")
                
                # è·å–CASTæ•°æ®
                cast_ratios = self._get_sector_cast_data(cast_data, sector_idx)
                
                # è·å–XSILMRæ•°æ®
                side_key = self._get_side_key(sector_idx)
                if side_key not in receiver_data:
                    self.logger.warning(f"æ¥æ”¶å™¨{receiver_id}ç¼ºå°‘{side_key}æ•°æ®")
                    continue
                
                xsilmr_waves = receiver_data[side_key]
                
                # å¤„ç†æ·±åº¦ç‚¹
                n_depths = xsilmr_waves.shape[1]
                if self.config['max_depths_per_sector']:
                    n_depths = min(n_depths, self.config['max_depths_per_sector'])
                
                sample_count += self._process_depth_points(
                    xsilmr_waves, depths, cast_ratios, receiver_id, sector_idx,
                    sample_count, batch_processor, signal_processor, n_depths
                )
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
        
        return sample_count
    
    def _get_sector_cast_data(self, cast_data, sector_idx):
        """è·å–æ‰‡åŒºå†…çš„CASTæ•°æ®"""
        n_sectors = self.config['azimuth_sectors']
        angle_per_sector = 360 // n_sectors
        
        start_angle = sector_idx * angle_per_sector
        end_angle = (sector_idx + 1) * angle_per_sector
        
        start_idx = start_angle // 2
        end_idx = end_angle // 2
        
        cast_zc = cast_data['Zc'][start_idx:end_idx, :]
        channeling_mask = cast_zc < 2.5
        channeling_ratios = np.mean(channeling_mask, axis=0)
        
        return channeling_ratios
    
    def _get_side_key(self, sector_idx):
        """æ ¹æ®æ‰‡åŒºç´¢å¼•è·å–æ•°æ®é”®"""
        # ç®€åŒ–æ˜ å°„ï¼šå¶æ•°æ‰‡åŒºç”¨SideAï¼Œå¥‡æ•°æ‰‡åŒºç”¨SideB
        return 'SideA' if sector_idx % 2 == 0 else 'SideB'
    
    def _process_depth_points(self, xsilmr_waves, depths, cast_ratios, receiver_id, 
                            sector_idx, base_sample_id, batch_processor, signal_processor, n_depths):
        """å¤„ç†æ·±åº¦ç‚¹"""
        processed_count = 0
        
        for depth_idx in range(n_depths):
            if depth_idx % 50 == 0 and depth_idx > 0:
                self.logger.info(f"    å¤„ç†æ·±åº¦ç‚¹ {depth_idx}/{n_depths}")
            
            try:
                # æå–æ³¢å½¢
                waveform = xsilmr_waves[:, depth_idx]
                
                # ä¿¡å·å¤„ç†
                filtered_waveform = signal_processor.apply_highpass_filter(waveform)
                scalogram = signal_processor.generate_scalogram(filtered_waveform)
                physical_features = signal_processor.extract_physical_features(filtered_waveform)
                
                # ç‰¹å¾å‘é‡
                vector_features = np.array([
                    physical_features['max_amplitude'],
                    physical_features['rms_amplitude'],
                    physical_features['energy'],
                    physical_features['zero_crossings'],
                    physical_features['dominant_frequency'],
                    physical_features['spectral_centroid'],
                    receiver_id,
                    sector_idx
                ], dtype=np.float32)
                
                # æ ‡ç­¾
                label = cast_ratios[depth_idx] if depth_idx < len(cast_ratios) else 0.0
                
                # å…ƒæ•°æ®
                metadata = (
                    depths[depth_idx],
                    receiver_id,
                    sector_idx,
                    base_sample_id + processed_count
                )
                
                # æ·»åŠ åˆ°æ‰¹å¤„ç†å™¨
                batch_processor.add_sample(
                    image_feature=scalogram,
                    vector_feature=vector_features,
                    label=label,
                    metadata=metadata
                )
                
                processed_count += 1
                
            except Exception as e:
                self.logger.warning(f"å¤„ç†æ·±åº¦ç‚¹{depth_idx}æ—¶å‡ºé”™: {e}")
                continue
        
        return processed_count
    
    def stage2_data_analysis(self, hdf5_path: str):
        """é˜¶æ®µ2: æ•°æ®åˆ†æ"""
        self.logger.info("\n" + "="*60)
        self.logger.info("é˜¶æ®µ2: æ•°æ®åˆ†æ")
        self.logger.info("="*60)
        
        from src.hdf5_manager import HDF5DataLoader
        from src.visualization import DataVisualizer
        
        # åŠ è½½æ•°æ®
        with HDF5DataLoader(hdf5_path) as loader:
            # å¯¼å‡ºæ‘˜è¦
            loader.manager.export_summary()
            
            # è·å–æ•°æ®ç»Ÿè®¡
            info = loader.manager.get_dataset_info()
            total_samples = loader.manager.get_total_samples()
            
            self.logger.info(f"æ•°æ®é›†ä¿¡æ¯:")
            self.logger.info(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
            self.logger.info(f"  å›¾åƒå½¢çŠ¶: {info.get('image_shape', 'N/A')}")
            self.logger.info(f"  å‘é‡ç»´åº¦: {info.get('vector_dim', 'N/A')}")
            
            # æ•°æ®å¯è§†åŒ–
            visualizer = DataVisualizer()
            
            # è¯»å–éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆ†æ
            sample_size = min(1000, total_samples)
            images, vectors, labels = loader.manager.datasets['images'][:sample_size], \
                                    loader.manager.datasets['vectors'][:sample_size], \
                                    loader.manager.datasets['labels'][:sample_size]
            
            # ç”Ÿæˆåˆ†æå›¾è¡¨
            fig_path = self.output_dir / "figures" / f"data_analysis_{self.data_scale}.png"
            visualizer.plot_data_analysis(
                images=images,
                vectors=vectors, 
                labels=labels,
                save_path=str(fig_path)
            )
            
            self.logger.info(f"æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜: {fig_path}")
            self.logger.info(f"æ•°æ®ç»Ÿè®¡:")
            self.logger.info(f"  çªœæ§½æ¯”ä¾‹å‡å€¼: {np.mean(labels):.4f}")
            self.logger.info(f"  çªœæ§½æ¯”ä¾‹æ ‡å‡†å·®: {np.std(labels):.4f}")
            self.logger.info(f"  é«˜çªœæ§½æ ·æœ¬ (>0.3): {np.sum(labels > 0.3)} ({np.sum(labels > 0.3)/len(labels)*100:.1f}%)")
    
    def stage3_model_training(self, hdf5_path: str):
        """é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ"""
        self.logger.info("\n" + "="*60)
        self.logger.info("é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ")
        self.logger.info("="*60)
        
        from src.model import HybridChannelingModel
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = HybridChannelingModel()
        
        # ä»HDF5è®­ç»ƒ
        self.logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        history = model.train_from_hdf5(
            hdf5_path=hdf5_path,
            epochs=self.config['epochs'],
            batch_size=self.config['batch_size'],
            test_size=0.2,
            val_size=0.1
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.output_dir / "models" / f"channeling_model_{self.data_scale}.h5"
        scaler_path = self.output_dir / "models" / f"scaler_{self.data_scale}.pkl"
        
        model.save_model(str(model_path), str(scaler_path))
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        self.logger.info(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
        
        return model, history
    
    def stage4_result_output(self, model, history, hdf5_path: str):
        """é˜¶æ®µ4: ç»“æœè¾“å‡º"""
        self.logger.info("\n" + "="*60)
        self.logger.info("é˜¶æ®µ4: ç»“æœè¾“å‡º")
        self.logger.info("="*60)
        
        from src.visualization import DataVisualizer
        from src.hdf5_manager import HDF5DataLoader
        
        visualizer = DataVisualizer()
        
        # 1. è®­ç»ƒå†å²å›¾
        history_fig = self.output_dir / "figures" / f"training_history_{self.data_scale}.png"
        model.plot_training_history(str(history_fig))
        self.logger.info(f"è®­ç»ƒå†å²å›¾å·²ä¿å­˜: {history_fig}")
        
        # 2. æ¨¡å‹é¢„æµ‹ç»“æœ
        with HDF5DataLoader(hdf5_path) as loader:
            # è¯»å–æµ‹è¯•æ•°æ®
            total_samples = loader.manager.get_total_samples()
            test_size = min(200, total_samples // 5)  # å–20%ä½œä¸ºæµ‹è¯•ï¼Œæœ€å¤š200ä¸ªæ ·æœ¬
            
            images = loader.manager.datasets['images'][-test_size:]
            vectors = loader.manager.datasets['vectors'][-test_size:]
            labels = loader.manager.datasets['labels'][-test_size:]
            
            # é¢„å¤„ç†
            images_processed = images[..., np.newaxis]  # æ·»åŠ é€šé“ç»´åº¦
            
            # é€æ ·æœ¬å½’ä¸€åŒ–
            for i in range(len(images_processed)):
                max_val = np.max(images_processed[i])
                if max_val > 1e-8:
                    images_processed[i] = images_processed[i] / max_val
            
            # æ ‡å‡†åŒ–å‘é‡ç‰¹å¾
            vectors_processed = model.scaler.transform(vectors)
            
            # é¢„æµ‹
            predictions = model.model.predict([images_processed, vectors_processed], verbose=0)
            predictions = predictions.flatten()
            
            # ç»˜åˆ¶é¢„æµ‹ç»“æœ
            pred_fig = self.output_dir / "figures" / f"predictions_{self.data_scale}.png"
            model.plot_predictions(labels, predictions, str(pred_fig))
            self.logger.info(f"é¢„æµ‹ç»“æœå›¾å·²ä¿å­˜: {pred_fig}")
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            mse = np.mean((labels - predictions) ** 2)
            mae = np.mean(np.abs(labels - predictions))
            rmse = np.sqrt(mse)
            
            self.logger.info("æ¨¡å‹è¯„ä¼°ç»“æœ:")
            self.logger.info(f"  å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
            self.logger.info(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
            self.logger.info(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.6f}")
        
        # 3. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        self._generate_final_report()
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report_path = self.output_dir / f"pipeline_report_{self.data_scale}.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("çªœæ§½æ£€æµ‹å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®è§„æ¨¡: {self.data_scale} - {self.config['description']}\n")
            f.write(f"æ·±åº¦èŒƒå›´: {self.depth_range[0]}-{self.depth_range[1]} ft\n")
            f.write(f"è¾“å‡ºç›®å½•: {self.output_dir}\n")
            f.write(f"æ—¥å¿—æ–‡ä»¶: {self.log_file}\n\n")
            
            f.write("ç”Ÿæˆçš„æ–‡ä»¶:\n")
            f.write("1. æ•°æ®æ–‡ä»¶:\n")
            for file in (self.output_dir / "data").glob("*.h5"):
                f.write(f"   - {file.name}: {file.stat().st_size / (1024**2):.2f} MB\n")
            
            f.write("\n2. æ¨¡å‹æ–‡ä»¶:\n")
            for file in (self.output_dir / "models").glob("*"):
                f.write(f"   - {file.name}\n")
            
            f.write("\n3. å›¾è¡¨æ–‡ä»¶:\n")
            for file in (self.output_dir / "figures").glob("*.png"):
                f.write(f"   - {file.name}\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("æµæ°´çº¿æ‰§è¡Œå®Œæˆ!\n")
            f.write("=" * 80 + "\n")
        
        self.logger.info(f"æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çªœæ§½æ£€æµ‹å®Œæ•´æµæ°´çº¿")
    parser.add_argument("--scale", choices=["small", "medium", "large"], default="small",
                       help="æ•°æ®è§„æ¨¡ (default: small)")
    parser.add_argument("--depth-min", type=float, default=2850.0,
                       help="æœ€å°æ·±åº¦ (ft) (default: 2850.0)")
    parser.add_argument("--depth-max", type=float, default=2950.0,
                       help="æœ€å¤§æ·±åº¦ (ft) (default: 2950.0)")
    parser.add_argument("--output-dir", default="outputs",
                       help="è¾“å‡ºç›®å½• (default: outputs)")
    
    args = parser.parse_args()
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    setup_chinese_font()
    
    # åˆ›å»ºå¹¶è¿è¡Œæµæ°´çº¿
    pipeline = CompletePipeline(
        depth_range=(args.depth_min, args.depth_max),
        data_scale=args.scale,
        output_dir=args.output_dir
    )
    
    success = pipeline.run_complete_pipeline()
    
    if success:
        print("\n" + "ğŸ‰" * 20)
        print("  çªœæ§½æ£€æµ‹å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ!")
        print("ğŸ‰" * 20)
        return 0
    else:
        print("\n" + "âŒ" * 20)
        print("  æµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        print("âŒ" * 20)
        return 1


if __name__ == "__main__":
    sys.exit(main()) 