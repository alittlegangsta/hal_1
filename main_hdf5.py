#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹äº•æ•°æ®çªœæ§½æ£€æµ‹é¡¹ç›® - HDF5å¢é‡å¤„ç†ä¸»ç¨‹åº
ä½¿ç”¨å†…å­˜é«˜æ•ˆçš„HDF5å­˜å‚¨æ–¹æ¡ˆå¤„ç†å¤§è§„æ¨¡æµ‹äº•æ•°æ®
æ·±åº¦èŒƒå›´ï¼š2850-2950ft (100ftèŒƒå›´ä»¥å¿«é€Ÿè·å¾—ç»“æœ)
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging
import time
import gc
from typing import Dict, List, Tuple

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('src')

from src.feature_engineering import IncrementalFeatureEngineer
from src.model import HybridChannelingModel
from src.hdf5_manager import HDF5DataManager
from src.visualization import DataVisualizer
from src.plot_config import setup_matplotlib

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hdf5_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def main():
    """ä¸»å¤„ç†æµç¨‹"""
    print("="*80)
    print("ğŸš€ æµ‹äº•æ•°æ®çªœæ§½æ£€æµ‹é¡¹ç›® - HDF5å¢é‡å¤„ç†")
    print("="*80)
    print("ğŸ“‹ å¤„ç†èŒƒå›´: 2850-2950ft (100ftç²¾ç¡®åˆ†æ)")
    print("ğŸ’¾ æŠ€æœ¯æ–¹æ¡ˆ: HDF5å¢é‡å­˜å‚¨ + å†…å­˜é«˜æ•ˆè®­ç»ƒ")
    print("âš¡ ä¼˜åŒ–ç›®æ ‡: å¿«é€Ÿç»“æœ + å†…å­˜å‹å¥½")
    print("="*80)
    
    # é…ç½®matplotlib
    setup_matplotlib()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dirs = ['data/processed', 'outputs/models', 'outputs/figures', 'outputs/logs']
    for dir_path in output_dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    # é…ç½®å‚æ•°
    config = {
        'depth_range': (2850.0, 2950.0),  # 100ftç²¾ç¡®èŒƒå›´
        'azimuth_sectors': 8,
        'batch_size': 50,
        'hdf5_path': 'data/processed/features_2850_2950.h5',
        'model_epochs': 50,  # å‡å°‘epochæ•°ä»¥å¿«é€Ÿè·å¾—ç»“æœ
        'train_batch_size': 32
    }
    
    logger.info(f"å¤„ç†é…ç½®: {config}")
    
    try:
        # æ­¥éª¤1: å¢é‡ç‰¹å¾å·¥ç¨‹
        print("\n" + "="*60)
        print("ğŸ“Š æ­¥éª¤1: å¢é‡ç‰¹å¾å·¥ç¨‹å¤„ç†")
        print("="*60)
        
        start_time = time.time()
        
        feature_engineer = IncrementalFeatureEngineer(
            depth_range=config['depth_range'],
            azimuth_sectors=config['azimuth_sectors'],
            batch_size=config['batch_size']
        )
        
        total_samples = feature_engineer.generate_features_to_hdf5(config['hdf5_path'])
        
        feature_time = time.time() - start_time
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {total_samples} æ ·æœ¬, è€—æ—¶ {feature_time:.1f}ç§’")
        
        # æ£€æŸ¥HDF5æ–‡ä»¶ä¿¡æ¯
        print_hdf5_info(config['hdf5_path'])
        
        # æ­¥éª¤2: æ¨¡å‹è®­ç»ƒ
        print("\n" + "="*60)
        print("ğŸ¤– æ­¥éª¤2: æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
        print("="*60)
        
        start_time = time.time()
        
        model = HybridChannelingModel(
            image_shape=(127, 1024),
            vector_dim=8
        )
        
        # ä»HDF5æ–‡ä»¶è®­ç»ƒæ¨¡å‹
        history = model.train_from_hdf5(
            hdf5_path=config['hdf5_path'],
            epochs=config['model_epochs'],
            batch_size=config['train_batch_size'],
            test_size=0.2,
            val_size=0.1
        )
        
        train_time = time.time() - start_time
        logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ: è€—æ—¶ {train_time:.1f}ç§’")
        
        # æ­¥éª¤3: ç»“æœå¯è§†åŒ–å’Œåˆ†æ
        print("\n" + "="*60)
        print("ğŸ“ˆ æ­¥éª¤3: ç»“æœåˆ†æå’Œå¯è§†åŒ–")
        print("="*60)
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        model.plot_training_history('outputs/figures/hdf5_training_history.png')
        
        # æ•°æ®é›†ç»Ÿè®¡åˆ†æ
        analyze_dataset_statistics(config['hdf5_path'])
        
        # æ¨¡å‹æ€§èƒ½åˆ†æ
        evaluate_model_performance(model, config['hdf5_path'])
        
        # æ­¥éª¤4: ä¿å­˜æ¨¡å‹
        print("\n" + "="*60)
        print("ğŸ’¾ æ­¥éª¤4: ä¿å­˜æ¨¡å‹å’Œç»“æœ")
        print("="*60)
        
        model.save_model(
            'outputs/models/hdf5_channeling_model.h5',
            'outputs/models/hdf5_scaler.pkl'
        )
        
        # å¯¼å‡ºHDF5æ•°æ®é›†æ‘˜è¦
        hdf5_manager = HDF5DataManager(config['hdf5_path'], mode='r')
        hdf5_manager.export_summary('outputs/logs/hdf5_dataset_summary.txt')
        
        # æ€»ç»“æŠ¥å‘Š
        print("\n" + "="*80)
        print("âœ… HDF5å¢é‡å¤„ç†å®Œæˆï¼")
        print("="*80)
        
        total_time = feature_time + train_time
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   â€¢ æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"   â€¢ æ·±åº¦èŒƒå›´: {config['depth_range'][0]}-{config['depth_range'][1]}ft")
        print(f"   â€¢ ç‰¹å¾å·¥ç¨‹è€—æ—¶: {feature_time:.1f}ç§’")
        print(f"   â€¢ æ¨¡å‹è®­ç»ƒè€—æ—¶: {train_time:.1f}ç§’")
        print(f"   â€¢ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        hdf5_info = HDF5DataManager(config['hdf5_path'], mode='r').get_data_info()
        print(f"   â€¢ HDF5æ–‡ä»¶å¤§å°: {hdf5_info['file_size_mb']:.1f}MB")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   â€¢ HDF5æ•°æ®é›†: {config['hdf5_path']}")
        print(f"   â€¢ è®­ç»ƒæ¨¡å‹: outputs/models/hdf5_channeling_model.h5")
        print(f"   â€¢ æ ‡å‡†åŒ–å™¨: outputs/models/hdf5_scaler.pkl")
        print(f"   â€¢ è®­ç»ƒå›¾è¡¨: outputs/figures/hdf5_training_history.png")
        print(f"   â€¢ æ•°æ®æ‘˜è¦: outputs/logs/hdf5_dataset_summary.txt")
        
        print(f"\nğŸ¯ ä¸»è¦ä¼˜åŠ¿:")
        print(f"   âœ“ å†…å­˜é«˜æ•ˆ: å¢é‡å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º")
        print(f"   âœ“ å¯æ‰©å±•æ€§: æ”¯æŒä»»æ„å¤§å°çš„æ•°æ®é›†")
        print(f"   âœ“ æ•°æ®å‹ç¼©: HDF5æ ¼å¼èŠ‚çœå­˜å‚¨ç©ºé—´")
        print(f"   âœ“ å¿«é€Ÿè®­ç»ƒ: ç›´æ¥ä»ç£ç›˜æµå¼è¯»å–")
        print(f"   âœ“ å¯å¤ç°æ€§: å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹")
        
        return True
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # æ¸…ç†å†…å­˜
        gc.collect()

def print_hdf5_info(hdf5_path: str):
    """æ‰“å°HDF5æ–‡ä»¶ä¿¡æ¯"""
    print(f"\nğŸ“‹ HDF5æ•°æ®é›†ä¿¡æ¯:")
    print("-" * 40)
    
    hdf5_manager = HDF5DataManager(hdf5_path, mode='r')
    info = hdf5_manager.get_data_info()
    
    print(f"æ–‡ä»¶è·¯å¾„: {hdf5_path}")
    print(f"æ–‡ä»¶å¤§å°: {info['file_size_mb']:.2f} MB")
    print(f"æ€»æ ·æœ¬æ•°: {info['total_samples']:,}")
    print(f"å›¾åƒç‰¹å¾å½¢çŠ¶: {info['image_shape']}")
    print(f"æ•°å€¼ç‰¹å¾ç»´åº¦: {info['vector_dim']}")
    print(f"æ•°æ®é›†åˆ—è¡¨: {', '.join(info['datasets'])}")
    
    # è®¡ç®—å‹ç¼©ç‡
    uncompressed_size = (
        info['total_samples'] * (
            np.prod(info['image_shape']) * 4 +  # å›¾åƒç‰¹å¾ (float32)
            info['vector_dim'] * 4 +            # æ•°å€¼ç‰¹å¾ (float32)
            4                                   # æ ‡ç­¾ (float32)
        )
    ) / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    
    compression_ratio = uncompressed_size / info['file_size_mb']
    print(f"å‹ç¼©ç‡: {compression_ratio:.1f}x ({uncompressed_size:.1f}MB -> {info['file_size_mb']:.1f}MB)")

def analyze_dataset_statistics(hdf5_path: str):
    """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡åˆ†æ:")
    print("-" * 40)
    
    hdf5_manager = HDF5DataManager(hdf5_path, mode='r')
    
    # è¯»å–å°‘é‡æ•°æ®è¿›è¡Œç»Ÿè®¡åˆ†æ
    sample_size = min(1000, hdf5_manager.get_data_info()['total_samples'])
    sample_data = hdf5_manager.read_batch(0, sample_size)
    
    labels = sample_data['labels']
    vector_features = sample_data['vector_features']
    
    print(f"æ ‡ç­¾ç»Ÿè®¡ (åŸºäº{sample_size}ä¸ªæ ·æœ¬):")
    print(f"  å¹³å‡çªœæ§½æ¯”ä¾‹: {np.mean(labels):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(labels):.3f}")
    print(f"  æœ€å°å€¼: {np.min(labels):.3f}")
    print(f"  æœ€å¤§å€¼: {np.max(labels):.3f}")
    print(f"  ä¸­ä½æ•°: {np.median(labels):.3f}")
    
    # çªœæ§½ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
    mild_ratio = np.mean(labels >= 0.3)
    moderate_ratio = np.mean(labels >= 0.5)
    severe_ratio = np.mean(labels >= 0.7)
    
    print(f"çªœæ§½ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    print(f"  è½»åº¦çªœæ§½ (â‰¥30%): {mild_ratio:.1%}")
    print(f"  ä¸­åº¦çªœæ§½ (â‰¥50%): {moderate_ratio:.1%}")
    print(f"  é‡åº¦çªœæ§½ (â‰¥70%): {severe_ratio:.1%}")
    
    print(f"æ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
    feature_names = ['max_amplitude', 'rms_amplitude', 'energy', 'zero_crossings',
                    'dominant_frequency', 'spectral_centroid', 'receiver_id', 'sector_id']
    
    for i, name in enumerate(feature_names):
        if i < vector_features.shape[1]:
            values = vector_features[:, i]
            print(f"  {name}: å‡å€¼={np.mean(values):.2e}, æ ‡å‡†å·®={np.std(values):.2e}")

def evaluate_model_performance(model: HybridChannelingModel, hdf5_path: str):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\nğŸ¯ æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
    print("-" * 40)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
    from src.model import HDF5DataGenerator
    
    hdf5_manager = HDF5DataManager(hdf5_path, mode='r')
    total_samples = hdf5_manager.get_data_info()['total_samples']
    
    # ä½¿ç”¨æœ€å20%çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†
    test_size = int(total_samples * 0.2)
    test_indices = list(range(total_samples - test_size, total_samples))
    
    test_generator = HDF5DataGenerator(hdf5_path, test_indices, batch_size=32, shuffle=False)
    
    # è¯„ä¼°æ¨¡å‹
    test_metrics = model.model.evaluate(test_generator, verbose=0)
    
    print("æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    for name, value in zip(model.model.metrics_names, test_metrics):
        print(f"  {name}: {value:.4f}")
    
    # è·å–é¢„æµ‹ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ
    print("æ­£åœ¨ç”Ÿæˆé¢„æµ‹ç»“æœç”¨äºè¯¦ç»†åˆ†æ...")
    
    # è¯»å–ä¸€å°æ‰¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹åˆ†æ
    test_batch = hdf5_manager.read_batch(total_samples - 100, 100)
    
    # é¢„å¤„ç†å›¾åƒæ•°æ®
    image_features = test_batch['image_features']
    image_features = image_features[..., np.newaxis]
    for i in range(len(image_features)):
        max_val = np.max(image_features[i])
        if max_val > 1e-8:
            image_features[i] = image_features[i] / max_val
    
    # é¢„å¤„ç†æ•°å€¼ç‰¹å¾
    vector_features = model.scaler.transform(test_batch['vector_features'])
    
    # é¢„æµ‹
    y_pred = model.model.predict([image_features, vector_features], verbose=0).flatten()
    y_true = test_batch['labels']
    
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
    model.plot_predictions(y_true, y_pred, 'outputs/figures/hdf5_predictions.png')
    
    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ (åŸºäº{len(y_true)}ä¸ªæµ‹è¯•æ ·æœ¬):")
    print(f"  MSE: {mse:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RÂ²: {r2:.4f}")
    
    # é¢„æµ‹ç²¾åº¦åˆ†æ
    abs_errors = np.abs(y_pred - y_true)
    print(f"é¢„æµ‹ç²¾åº¦åˆ†æ:")
    print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {np.mean(abs_errors):.3f}")
    print(f"  è¯¯å·®æ ‡å‡†å·®: {np.std(abs_errors):.3f}")
    print(f"  90%æ ·æœ¬è¯¯å·® < {np.percentile(abs_errors, 90):.3f}")
    print(f"  95%æ ·æœ¬è¯¯å·® < {np.percentile(abs_errors, 95):.3f}")

def demo_hdf5_capabilities():
    """æ¼”ç¤ºHDF5å¢é‡å¤„ç†çš„æ ¸å¿ƒåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ”§ HDF5å¢é‡å¤„ç†æŠ€æœ¯æ¼”ç¤º")
    print("="*60)
    
    demo_path = "data/processed/demo_features.h5"
    
    # åˆ›å»ºæ¼”ç¤ºç”¨çš„å°è§„æ¨¡HDF5æ–‡ä»¶
    print("åˆ›å»ºæ¼”ç¤ºHDF5æ•°æ®é›†...")
    demo_manager = HDF5DataManager(demo_path, mode='w')
    demo_manager.create_dataset_structure(
        total_samples=1000,
        image_shape=(127, 1024),
        vector_dim=8,
        chunk_size=100
    )
    
    # æ¼”ç¤ºæ‰¹é‡å†™å…¥
    print("æ¼”ç¤ºæ‰¹é‡æ•°æ®å†™å…¥...")
    for i in range(0, 1000, 100):
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        batch_images = np.random.random((100, 127, 1024)).astype(np.float32)
        batch_vectors = np.random.random((100, 8)).astype(np.float32)
        batch_labels = np.random.random(100).astype(np.float32)
        
        demo_manager.write_batch(i, batch_images, batch_vectors, batch_labels)
        print(f"  å·²å†™å…¥æ‰¹æ¬¡ {i//100 + 1}/10")
    
    # æ¼”ç¤ºæ•°æ®è¯»å–
    print("æ¼”ç¤ºæ•°æ®è¯»å–...")
    demo_manager.mode = 'r'
    
    # è¯»å–éƒ¨åˆ†æ•°æ®
    batch_data = demo_manager.read_batch(0, 50)
    print(f"  è¯»å–50ä¸ªæ ·æœ¬: å›¾åƒå½¢çŠ¶={batch_data['image_features'].shape}")
    
    # åˆ›å»ºæ•°æ®è¿­ä»£å™¨
    print("æ¼”ç¤ºæ•°æ®è¿­ä»£å™¨...")
    iterator = demo_manager.create_data_iterator(batch_size=32, shuffle=True)
    for i, batch in enumerate(iterator):
        if i >= 3:  # åªæ¼”ç¤ºå‰3ä¸ªæ‰¹æ¬¡
            break
        print(f"  æ‰¹æ¬¡{i+1}: å›¾åƒ={batch['image_features'].shape}, æ ‡ç­¾={batch['labels'].shape}")
    
    # æ¸…ç†æ¼”ç¤ºæ–‡ä»¶
    Path(demo_path).unlink(missing_ok=True)
    print("æ¼”ç¤ºå®Œæˆï¼Œå·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")

if __name__ == "__main__":
    print("æµ‹äº•æ•°æ®HDF5å¢é‡å¤„ç†ç³»ç»Ÿ")
    print("="*50)
    
    # å¯é€‰: è¿è¡ŒæŠ€æœ¯æ¼”ç¤º
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == '--demo':
        demo_hdf5_capabilities()
    
    # è¿è¡Œä¸»å¤„ç†æµç¨‹
    success = main()
    
    if success:
        print("\nğŸ‰ å¤„ç†æˆåŠŸå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼")
        sys.exit(1) 